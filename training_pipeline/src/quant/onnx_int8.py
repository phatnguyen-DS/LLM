import torch
from transformers import AutoModelForSequenceClassification, AutoTokenizer
import os
import onnxruntime
from onnxruntime.quantization import quantize_matmul_4bits
import warnings

# Táº¯t cáº£nh bÃ¡o
warnings.filterwarnings("ignore")

print(f"ğŸ’» Äang cháº¡y trÃªn Windows - Bypass Optimum...")

# --- 1. Cáº¤U HÃŒNH ÄÆ¯á»œNG DáºªN CHÃNH XÃC ---
# Dá»±a trÃªn Ä‘Æ°á»ng dáº«n trong áº£nh báº¡n gá»­i
BASE_DIR = r"C:\Users\TAN PHAT\OneDrive\Desktop\LLM\models"

INPUT_PATH = os.path.join(BASE_DIR, "raw_pytorch")     # Folder chá»©a model gá»‘c
TEMP_PATH = os.path.join(BASE_DIR, "onnx_temp")        # Folder lÆ°u táº¡m
OUTPUT_PATH = os.path.join(BASE_DIR, "production_int4") # Folder thÃ nh pháº©m

# Táº¡o thÆ° má»¥c
os.makedirs(TEMP_PATH, exist_ok=True)
os.makedirs(OUTPUT_PATH, exist_ok=True)

# --- 2. EXPORT DÃ™NG PYTORCH THUáº¦N (KHÃ”NG DÃ™NG OPTIMUM) ---
print(f"â³ Äang load model tá»«: {INPUT_PATH}")
try:
    # Load model lÃªn CPU
    model = AutoModelForSequenceClassification.from_pretrained(INPUT_PATH).cpu()
    tokenizer = AutoTokenizer.from_pretrained(INPUT_PATH)
    model.eval()
except Exception as e:
    print(f"\nâŒ Lá»–I LOAD MODEL: {e}")
    print(f"ğŸ‘‰ HÃ£y kiá»ƒm tra láº¡i folder '{INPUT_PATH}' cÃ³ file model.safetensors hoáº·c pytorch_model.bin khÃ´ng.")
    exit()

print("ğŸ”„ Äang export sang ONNX (Float32)...")
# Táº¡o input giáº£
dummy_text = "Chuyá»ƒn Ä‘á»•i model sang onnx"
inputs = tokenizer(dummy_text, return_tensors="pt")

onnx_float_file = os.path.join(TEMP_PATH, "model.onnx")

# DÃ¹ng torch.onnx.export (TÃ­nh nÄƒng cÃ³ sáºµn cá»§a PyTorch, cá»±c ká»³ á»•n Ä‘á»‹nh)
torch.onnx.export(
    model,
    (inputs['input_ids'], inputs['attention_mask']),
    onnx_float_file,
    input_names=['input_ids', 'attention_mask'],
    output_names=['logits'],
    dynamic_axes={
        'input_ids': {0: 'batch_size', 1: 'sequence_length'},
        'attention_mask': {0: 'batch_size', 1: 'sequence_length'},
        'logits': {0: 'batch_size'}
    },
    opset_version=14 
)
print("âœ… Export ONNX gá»‘c thÃ nh cÃ´ng!")

# --- 3. NÃ‰N INT4 (DÃ™NG ONNXRUNTIME) ---
print("ğŸ”¨ Äang nÃ©n model xuá»‘ng Int4...")
final_model_file = os.path.join(OUTPUT_PATH, "model_int4.onnx")

try:
    quantize_matmul_4bits(
        onnx_float_file,
        final_model_file,
        block_size=32,
        is_symmetric=True
    )
except Exception as e:
    print(f"âŒ Lá»—i khi nÃ©n: {e}")
    exit()

# Copy tokenizer sang Ä‘Ã­ch
tokenizer.save_pretrained(OUTPUT_PATH)

print("-" * 50)
print("ğŸ‰ THÃ€NH CÃ”NG! Báº N ÄÃƒ CÃ“ MODEL INT4.")
print(f"ğŸ“‚ ThÆ° má»¥c deploy: {OUTPUT_PATH}")
print("-" * 50)