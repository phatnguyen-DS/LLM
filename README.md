# Text Classification LLM

Dá»± Ã¡n phÃ¢n loáº¡i vÄƒn báº£n tiáº¿ng Viá»‡t sá»­ dá»¥ng mÃ´ hÃ¬nh Transformer, triá»ƒn khai vá»›i kiáº¿n trÃºc microservices vÃ  tá»‘i Æ°u hÃ³a hiá»‡u suáº¥t vá»›i ONNX.

## ğŸš€ Overview

Dá»± Ã¡n nÃ y lÃ  má»™t há»‡ thá»‘ng phÃ¢n loáº¡i vÄƒn báº£n hoÃ n chá»‰nh tá»« end-to-end, Ä‘Æ°á»£c xÃ¢y dá»±ng vá»›i kiáº¿n trÃºc microservices, bao gá»“m:
- **Backend API**: FastAPI vá»›i ONNX Runtime Ä‘á»ƒ phá»¥c vá»¥ inference hiá»‡u suáº¥t cao
- **Frontend UI**: Streamlit cho giao diá»‡n ngÆ°á»i dÃ¹ng tÆ°Æ¡ng tÃ¡c
- **Training Pipeline**: Quy trÃ¬nh MLOps hoÃ n chá»‰nh tá»« xá»­ lÃ½ dá»¯ liá»‡u Ä‘áº¿n huáº¥n luyá»‡n vÃ  triá»ƒn khai
- **Model Optimization**: Quantization vÃ  tá»‘i Æ°u hÃ³a model Ä‘á»ƒ giáº£m kÃ­ch thÆ°á»›c vÃ  tÄƒng tá»‘c Ä‘á»™ inference

## ğŸ“ Cáº¥u trÃºc thÆ° má»¥c

```
text-classification-llm/
â”‚
â”œâ”€â”€ .github/                   # CI/CD workflows (sáº½ Ä‘Æ°á»£c thÃªm)
â”œâ”€â”€ .gitignore                 # Ignore venv, __pycache__, models náº·ng (.bin)
â”œâ”€â”€ README.md                  # TÃ i liá»‡u dá»± Ã¡n
â”‚
â”œâ”€â”€ data/                      # --- QUáº¢N LÃ Dá»® LIá»†U ---
â”‚   â”œâ”€â”€ raw/                   # Dá»¯ liá»‡u thÃ´ (csv, excel) chÆ°a xá»­ lÃ½
â”‚   â”‚   â””â”€â”€ banking_text.csv   # Dataset vÄƒn báº£n ngÃ¢n hÃ ng tiáº¿ng Viá»‡t
â”‚   â”œâ”€â”€ processed/             # Dá»¯ liá»‡u Ä‘Ã£ lÃ m sáº¡ch (dÃ¹ng Ä‘á»ƒ train)
â”‚   â”‚   â”œâ”€â”€ train.csv          # Dá»¯ liá»‡u huáº¥n luyá»‡n
â”‚   â”‚   â”œâ”€â”€ val.csv            # Dá»¯ liá»‡u validation
â”‚   â”‚   â””â”€â”€ test.csv           # Dá»¯ liá»‡u test
â”‚
â”œâ”€â”€ models/                    # --- MODEL ARTIFACTS ---
â”‚   â”œâ”€â”€ raw_model/             # Model PyTorch sau khi fine-tune
â”‚   â”œâ”€â”€ onnx_int8/             # Model ONNX Int8 (táº¡m thá»i)
â”‚   â””â”€â”€ production/            # Model ONNX Int8 (sáºµn sÃ ng deploy)
â”‚       â”œâ”€â”€ model_main.onnx    # Model chÃ­nh
â”‚       â”œâ”€â”€ tokenizer.json     # Tokenizer
â”‚       â””â”€â”€ config.json        # Cáº¥u hÃ¬nh model
â”‚
â”œâ”€â”€ training_pipeline/         # --- PIPELINE TRAINING ---
â”‚   â”œâ”€â”€ requirements.txt       # Dependencies cho training
â”‚   â”‚
â”‚   â”œâ”€â”€ notebooks/             # Jupyter Notebooks
â”‚   â”‚   â””â”€â”€ 01_eda_analysis.ipynb  # PhÃ¢n tÃ­ch dá»¯ liá»‡u
â”‚   â”‚
â”‚   â””â”€â”€ src/                   # Source code xá»­ lÃ½ logic
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ cleaning/          # Module lÃ m sáº¡ch dá»¯ liá»‡u
â”‚       â”‚   â”œâ”€â”€ __init__.py
â”‚       â”‚   â””â”€â”€ clean.py       # Script lÃ m sáº¡ch vÃ  split data
â”‚       â”œâ”€â”€ training/          # Module huáº¥n luyá»‡n model
â”‚       â”‚   â”œâ”€â”€ __init__.py
â”‚       â”‚   â””â”€â”€ train.py       # Script fine-tuning model
â”‚       â””â”€â”€ quant/             # Module quantization
â”‚           â”œâ”€â”€ __init__.py
â”‚           â”œâ”€â”€ onnx_int8.py   # Script quantize ONNX
â”‚           â””â”€â”€ convert.py     # Script chuyá»ƒn Ä‘á»•i model
â”‚
â”œâ”€â”€ backend/                   # --- SERVICE 1: FASTAPI ---
â”‚   â”œâ”€â”€ api.py                 # API endpoints
â”‚   â”œâ”€â”€ requirements.txt       # Dependencies cho inference
â”‚   â””â”€â”€ Dockerfile             # Dockerfile cho backend
â”‚
â””â”€â”€ frontend/                  # --- SERVICE 2: STREAMLIT ---
    â”œâ”€â”€ streamlit.py           # Giao diá»‡n ngÆ°á»i dÃ¹ng
    â”œâ”€â”€ requirements.txt       # Dependencies cho frontend
    â””â”€â”€ Dockerfile             # Dockerfile cho frontend
```

## ğŸ› ï¸ CÃ i Ä‘áº·t vÃ  sá»­ dá»¥ng

### YÃªu cáº§u
- Python 3.9+
- Docker (náº¿u cháº¡y vá»›i container)
- GPU (náº¿u huáº¥n luyá»‡n)

### 1. Huáº¥n luyá»‡n model má»›i

```bash
# Clone repository
git clone https://github.com/username/text-classification-llm.git
cd text-classification-llm

# CÃ i Ä‘áº·t dependencies cho training
cd training_pipeline
pip install -r requirements.txt

# Xá»­ lÃ½ dá»¯ liá»‡u
python src/cleaning/clean.py

# Huáº¥n luyá»‡n model
python src/training/train.py

# Quantize model
python src/quant/onnx_int8.py

# Chuyá»ƒn Ä‘á»•i model sang production
python src/quant/convert.py
```

### 2. Cháº¡y backend API

```bash
# CÃ i Ä‘áº·t dependencies
cd backend
pip install -r requirements.txt

# Cháº¡y API server
uvicorn api:app --host 0.0.0.0 --port 10000 --reload
```

### 3. Cháº¡y frontend UI

```bash
# CÃ i Ä‘áº·t dependencies
cd frontend
pip install -r requirements.txt

# Cháº¡y Streamlit app
streamlit run streamlit.py --server.port 8501
```

### 4. Sá»­ dá»¥ng Docker

```bash
# Build vÃ  cháº¡y backend
docker build -f backend/Dockerfile -t llm-backend .
docker run -p 10000:10000 llm-backend

# Build vÃ  cháº¡y frontend
docker build -f frontend/Dockerfile -t llm-frontend .
docker run -p 8501:8501 llm-frontend
```

### 5. Sá»­ dá»¥ng API

```python
import requests

# Gá»­i request Ä‘áº¿n API
response = requests.post(
    "http://localhost:10000/predict",
    json={"text": "Tháº» cá»§a tÃ´i bá»‹ lá»—i khÃ´ng thá»ƒ sá»­ dá»¥ng"}
)

# Xem káº¿t quáº£
result = response.json()
print(f"Label: {result['label']}")
print(f"Score: {result['score']}")
```

## ğŸ“Š ThÃ´ng sá»‘ ká»¹ thuáº­t

### Model
- **Base Model**: `sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2`
- **Architecture**: Transformer-based encoder
- **Classes**: 6
  - CARD_ISSUE: Váº¥n Ä‘á» liÃªn quan Ä‘áº¿n tháº»
  - APP_LOGIN: Váº¥n Ä‘á» Ä‘Äƒng nháº­p á»©ng dá»¥ng
  - TRANSACTION: Váº¥n Ä‘á» giao dá»‹ch
  - LOAN_SAVING: Váº¥n Ä‘á» vay/tiáº¿t kiá»‡m
  - FRAUD_REPORT: BÃ¡o cÃ¡o lá»«a Ä‘áº£o
  - OTHERS: CÃ¡c váº¥n Ä‘á» khÃ¡c
- **Max Sequence Length**: 64 tokens
- **Optimization**: Dynamic Quantization (INT8)

### Performance Metrics
- **Accuracy**: TBD (sáº½ Ä‘Æ°á»£c cáº­p nháº­t sau khi Ä‘Ã¡nh giÃ¡)
- **F1 Score**: TBD
- **Model Size**: ~650KB (sau quantization)
- **Inference Time**: <50ms (CPU)
- **Throughput**: TBD requests/second

## ğŸ—ï¸ Kiáº¿n trÃºc há»‡ thá»‘ng

### Microservices Architecture
- **Backend Service**: FastAPI vá»›i ONNX Runtime
  - Endpoint `/predict`: Dá»± Ä‘oÃ¡n lá»›p vÄƒn báº£n
  - Endpoint `/health`: Kiá»ƒm tra tráº¡ng thÃ¡i há»‡ thá»‘ng
- **Frontend Service**: Streamlit UI
  - Giao diá»‡n ngÆ°á»i dÃ¹ng thÃ¢n thiá»‡n
  - TÆ°Æ¡ng tÃ¡c vá»›i backend qua REST API

### Data Flow
1. NgÆ°á»i dÃ¹ng nháº­p vÄƒn báº£n vÃ o frontend
2. Frontend gá»­i request Ä‘áº¿n backend API
3. Backend tiá»n xá»­ lÃ½ text vÃ  chuyá»ƒn thÃ nh tokens
4. ONNX model thá»±c hiá»‡n inference
5. Backend tráº£ vá» káº¿t quáº£ cho frontend
6. Frontend hiá»ƒn thá»‹ káº¿t quáº£ cho ngÆ°á»i dÃ¹ng

### Model Deployment Pipeline
1. Raw Data â†’ Cleaned Data (clean.py)
2. Cleaned Data â†’ Trained Model (train.py)
3. Trained Model â†’ ONNX Model (onnx_int8.py)
4. ONNX Model â†’ Production Model (convert.py)
5. Production Model â†’ Containerized API (Docker)

## ğŸ”§ Best Practices vÃ  Optimizations

### Backend Optimizations
- Giá»›i háº¡n sá»‘ luá»“ng CPU Ä‘á»ƒ tá»‘i Æ°u tÃ i nguyÃªn
- Sá»­ dá»¥ng ONNX Runtime cho inference hiá»‡u suáº¥t cao
- Caching model vÃ  tokenizer Ä‘á»ƒ trÃ¡nh táº£i láº¡i nhiá»u láº§n
- Error handling vÃ  logging chi tiáº¿t
- FastAPI vá»›i automatic docs generation

### Frontend Features
- Responsive design cho nhiá»u thiáº¿t bá»‹
- Xá»­ lÃ½ lá»—i ngÆ°á»i dÃ¹ng thÃ¢n thiá»‡n
- Health check vÃ  status indicators
- VÃ­ dá»¥ máº«u Ä‘á»ƒ hÆ°á»›ng dáº«n ngÆ°á»i dÃ¹ng
- Non-blocking UI vá»›i loading states

### Model Optimizations
- Dynamic quantization Ä‘á»ƒ giáº£m kÃ­ch thÆ°á»›c model
- Multi-stage Docker builds Ä‘á»ƒ tá»‘i Æ°u image size
- Separate production and training environments

## ğŸš€ Triá»ƒn khai

### Local Development
```bash
# Backend
cd backend && uvicorn api:app --reload

# Frontend
cd frontend && streamlit run streamlit.py
```

### Production (Render)
- **Backend**: Deploy FastAPI service vá»›i Docker
- **Frontend**: Deploy Streamlit app vá»›i Docker
- **Database**: (Optional) Ghi log vÃ  metrics
- **Monitoring**: Health checks vÃ  uptime monitoring

### Image Sizes
- **Backend Image**: ~200MB (bao gá»“m model)
- **Frontend Image**: ~50MB
- **Total**: ~250MB (náº±m trong giá»›i háº¡n cá»§a Render Free)

## ğŸ“ Todo List (Middle Level Features)

### Testing
- [ ] Unit tests cho táº¥t cáº£ modules
- [ ] Integration tests cho API endpoints
- [ ] Model performance regression tests
- [ ] End-to-end tests cho toÃ n bá»™ pipeline

### Monitoring & Logging
- [ ] Structured logging vá»›i ELK stack
- [ ] Prometheus metrics cho performance
- [ ] Grafana dashboard visualization
- [ ] Alert system cho lá»—i vÃ  anomalies

### Security
- [ ] API authentication vá»›i JWT
- [ ] Rate limiting Ä‘á»ƒ báº£o vá»‡ API
- [ ] Input validation vÃ  sanitization
- [ ] HTTPS vÃ  secure headers

### CI/CD
- [ ] GitHub Actions cho automated testing
- [ ] Automated model validation
- [ ] Blue-green deployment strategy
- [ ] Rollback mechanisms

### Performance
- [ ] Redis caching cho frequent requests
- [ ] Batch processing cho multiple texts
- [ ] Model versioning and A/B testing
- [ ] Load balancing vÃ  horizontal scaling

## ğŸ¤ ÄÃ³ng gÃ³p

1. Fork repository
2. Táº¡o feature branch (`git checkout -b feature/AmazingFeature`)
3. Commit changes (`git commit -m 'Add some AmazingFeature'`)
4. Push to branch (`git push origin feature/AmazingFeature`)
5. Open Pull Request

## ğŸ“„ License

Dá»± Ã¡n nÃ y Ä‘Æ°á»£c cáº¥p phÃ©p theo MIT License - xem file [LICENSE](LICENSE) Ä‘á»ƒ biáº¿t chi tiáº¿t.

## ğŸ‘¥ Team

- **Lead AI Engineer**: [TÃªn]
- **ML Engineer**: [TÃªn]
- **Backend Developer**: [TÃªn]
- **Frontend Developer**: [TÃªn]

## ğŸ“ LiÃªn há»‡

- **Project Link**: [https://github.com/username/text-classification-llm](https://github.com/username/text-classification-llm)
- **Issues**: [https://github.com/username/text-classification-llm/issues](https://github.com/username/text-classification-llm/issues)